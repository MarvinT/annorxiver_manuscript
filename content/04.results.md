## Results

### Comparing bioRxiv to other corpora

#### bioRxiv Metadata Statistics

The preprint landscape is rapidly changing, and the number of bioRxiv preprints in our data download (71,118) was nearly double that of a recent study that reported on a snapshot with 37,648 preprints [@doi:10.7554/eLife.45133].
Because the rate of change is rapid, we first analyzed category data and compared our results with previous findings.
As in previous reports [@doi:10.7554/eLife.45133], neuroscience remains the most common category of preprint followed by bioinformatics (Supplemental Figure {@fig:biorxiv_categories}).
Microbiology, which was fifth in the most recent report [@doi:10.7554/eLife.45133], has now surpassed evolutionary biology and genomics to move into third.
When authors upload their preprints, they select from three result category types: new results, confirmatory results or contradictory results.
We found that nearly all preprints (97.5%) were categorized as new results, which is consistent with reports on a smaller set [@doi:10.1001/jama.2017.21168].
Taken together, the results suggest that while bioRxiv has experienced dramatic growth, the way in which it is being used appears to have remained consistent in recent years.

#### Global analysis reveals similarities and differences between bioRxiv and PMC

| Metric                | bioRxiv     | PMC           | NYTAC         |
|-----------------------|-------------|---------------|---------------|
| document count             | 71,118      | 1,977,647     | 1,855,658     |
| sentence count             | 22,195,739  | 480,489,811   | 72,171,037    |
| token count                | 420,969,930 | 8,597,101,167 | 1,218,673,384 |
| stopword count            | 158,429,441 | 3,153,077,263 | 559,391,073   |
| avg. document length        | 312.10      | 242.96        | 38.89         |
| avg. sentence length        | 22.71       | 21.46         | 19.89         |
| negatives                  | 1,148,382   | 24,928,801    | 7,272,401     |
| coordinating conjunctions  | 14,295,736  | 307,082,313   | 38,730,053    |
| coordinating conjunctions% | 3.40%       | 3.57%         | 3.18%         |
| pronouns                   | 4,604,432   | 74,994,125    | 46,712,553    |
| pronouns%                  | 1.09%        | 0.87%         | 3.83%         |
| passives                   | 15,012,441  | 342,407,363   | 19,472,053    |
| passive%                   | 3.57%       | 3.98%         | 1.60%         |

Table: Summary statistics for the bioRxiv, PMC, and NYTAC corpora. {#tbl:corpora_stats}

![
A. The Kullback–Leibler divergence measures the extent to which the distributions, not specific tokens, differ from each other.
The token distribution of bioRxiv and PMC corpora is more similar than these biomedical corpora are to the NYTAC one.
B. The major differences in token frequencies for the corpora appear to be driven by the fields that have had the highest uptake of bioRxiv, as terms from neuroscience and genomics are relatively more abundant in bioRxiv.
Points indicate the log_2(OR) for each token and error bars indicate the 95% confidence intervals.
C. Of the terms that differ between bioRxiv and PMC, the most abundant in bioRxiv are "et" and "al" while the most abundant in PMC is "study."
D. The major differences in token frequencies for preprints and their corresponding published version often appear to be associated with typesetting and supplementary or additional materials.
Points indicate the log_2(OR) for each token and error bars indicate the 95% confidence intervals.
E. The tokens with the largest absolute differences in abundance appear to be stylistic.
](https://raw.githubusercontent.com/greenelab/annorxiver/472d9b2abd42aecff339fe21c8378f9a556b80d8/figure_generation/output/figure_one_panels.svg){#fig:corpora_comparison_panels, width="100%"}

We first compared the overall text of bioRxiv with PMC, adding a corpus of professionally written but non-biomedical text (NYTAC) as a control.
Documents on bioRxiv were slightly longer than those on PMC, but both were much longer than those from NYTAC (Table {@tbl:corpora_stats}).
<!--IS THERE AN INCREASE IN PMC DOCUMENT LENGTH OVER TIME? ARE MODERN PAPERS THE SAME LENGTH AS PREPRINTS? Also could be a function of the field or similar. Don't think we need to answer this now - could leave it for the future unless we are asked to look into this.-->
Other than length, both corpora were otherwise quite similar.
The average sentence length, fraction of pronouns, and the use of the passive voice were all more similar between bioRxiv and PMC than they were to NYTAC (Table {@tbl:corpora_stats}).
The Kullback–Leibler divergence measures the extent to which two distributions, but not the specific entities that comprise those distributions, differ.
The distribution of term frequencies in bioRxiv and PMC were low, especially among the top few hundred tokens (Figure {@fig:corpora_comparison_panels}A).
Differences began to emerge over more terms, but remained much lower than when the biomedical corpora are compared with NYTAC.

Examining the frequencies of individual terms revealed differences between the biomedical corpora.
Previous work examining author-selected categories has reported that fields appear to have preprinted unevenly, with certain life sciences research fields having more uptake than others [@doi:10.7554/eLife.45133].
However, it was possible that authors simply selected certain fields preferentially but that the content was similar to the broader corpus of life sciences text.
We directly examined this by comparing term frequencies between bioRxiv and PMC.
We found that among the terms that differed the most <!-- HOW DID YOU PICK THE TERMS FOR THE PLOT? I THINK IT WAS A COMBO OF FREQUENCY AND OR --> were many associated with fields.
Terms like "neurons" "genome" and "genetic", which are common in genomics and neuroscience, were more common in bioRxiv than PMC while others associated with clinical research, such as "clinical" "patients" and "treatment" were more common in PMC (Figure {@fig:corpora_comparison_panels}B and {@fig:corpora_comparison_panels}C).

We next controlled for differences in the body of documents to identify term-level changes associated with the publication process itself by examining only pairs of preprints and their corresponding publication (Figure {@fig:corpora_comparison_panels}D and {@fig:corpora_comparison_panels}E).
The tokens that differed included "et" "al", "$\pm$", "–" and others that appeared to be typesetting related.
Certain changes appeared to be related to journal styles: "figure" was more common in bioRxiv while "fig" was relatively more common in PMC.
Other changes appeared to be associated with an increasing reference to content external to the manuscript itself: the tokens "supplementary", "additional" and "file" were all more common in PMC than bioRxiv suggesting that journals are not simply replacing one token with another but that there are more mentions of such content after peer review.

Taken together these results suggested that the structure of the text in documents on bioRxiv was similar to that on PMC.
The differences in uptake across fields is supported not only by differences in authors' categorization of their articles but also in the text of the articles themselves.
At the level of individual manuscripts, the terms that change the most appear to be associated with typesetting, journal style, and an increasing reliance on additional materials after peer review.

### Document embeddings derived from bioRxiv reveal field categories

![
A. Principal components (PC) analysis of bioRxiv word2vec embeddings groups documents by author-selected categories.
We visualized documents from key categories on a scatterplot for the first two PCs.
The first PC separated cell biology from informatics-related fields.
The second PC separated bioinformatics from neuroscience.
Certain neuroscience papers appeared to be more associated with the cellular biology direction of PC1, while others appeared to be more associated with the informatics-related direction, which suggested that the concepts captured by PCs were not exclusively related to field.
B. A word cloud visualization of PC1, which separated informatics disciplines (positive direction) from cell biology (negative direction) showed that tokens "empirical" "estimates" and "statistics" characterized the positive direction while "cultured" and "overexpressing" characterized the negative one.
Each word cloud depicts the cosine similarity score between tokens and the second PC.
Tokens in orange were most similar to the PC's positive direction while tokens in blue were most similar to the PC's negative direction.
The size of each token indicates the magnitude of the similarity.
C. A word cloud visualization of PC2, which separated bioinformatics from neuroscience, showed that tokens "genomic" "genome" and "genomes" characterized the positive direction while "evoked" "stimulus" and "stimulation" characterized the negative one.
D. Examining PC1 values for each article by category created a continuum from informatics-related fields on the top through cell biology on the bottom.
Certain article categories (neuroscience, genetics) were spread throughout PC1 values.
E. Examining PC2 values for each article by category revealed fields like genomics, bioinformatics, and genetics on the top and neuroscience and behavior on the bottom.
](https://raw.githubusercontent.com/greenelab/annorxiver/472d9b2abd42aecff339fe21c8378f9a556b80d8/figure_generation/output/figure_two_panels.svg){#fig:topic_analysis_panels width="100%"}

Document embeddings provide a means to categorize the language of documents in a way that takes into account the similarities between terms. <!-- Needs a ref or few? I know I've seen you present these papers. -->
We first trained word embeddings using a 300-dimensional word2vec continuous bag of words model.
We combined word embeddings to produce an embedding for each bioRxiv or PMC document by calculating the average embedding score.
This placed each document in a 300-dimensional space where the dimensions were arbitrary.
To provide more structure to the dataset, we examined the predominant patterns in these embeddings by performing principal components analysis of bioRxiv.
The principal components (PCs) are ordered by the proportion of the variance explained.
We found that the first two PCs separated articles from different author-selected categories (Figure {@fig:topic_analysis_panels}A).

We sought to understand the token patterns that drove these overall differences between documents.
We identified the tokens most similar to a PC by calculating the cosine similarity score between tokens' embeddings in the word2vec space and each PC.
Visualizing token-PC similarity revealed tokens associated with certain research approaches (Figures @fig:topic_analysis_panels}B and @fig:topic_analysis_panels}C).
Examining the value for PC1 across all author-selected categories revealed an ordering of fields from cell biology to informatics-related disciplines (Figure @fig:topic_analysis_panels}D).
While the PC1 value range for each author-selected category was high, these results suggested that a primary driver in the variability of language use on bioRxiv could be the divide between data science approaches and cell biology ones.
A similar analysis for PC2 suggested that neuroscience and genomics present a similar language continuum (Figure @fig:topic_analysis_panels}E).


We explored the primary differences between the full text of bioRxiv preprints by performing principal components analysis on generated document embeddings.
We visualized the correspondence between tokens and the loadings for each principal component (Figure {@fig:topic_analysis_panels}).
We also visualized documents projected on selected principal components .
The first principal component separates bioRxiv preprints that encompass molecular biology results with preprints that contain quantitative biology results (Figure {@fig:topic_analysis_panels}B).
This highlights the bisection of biomedical research where majority of results can be categorized under the molecular biology category or the quantitative biology category.
Furthermore, this bisecting trend is evident across individual preprint categories as most categories lie on either side of the first principal component (Figure {@fig:topic_analysis_panels}D).
As with the first principal component we also provide example preprints from the systems biology category to reinforce this concept (Supplemental Table {@tbl:five_pc1_table}).

The second principal component represents the concept of neuroscience vs bioinformatics (Figure {@fig:topic_analysis_panels}C).
This principal component suggests that the bulk of preprints within bioRxiv are largely focused around neuroscience and bioinformatic concepts.
This split is evident in Figure {@fig:topic_analysis_panels}E as enriched categories along this principal component are quite related to neuroscience (negative end) or bioinformatics (positive end).
We provide example preprints from the systems biology category to reinforce this concept (Supplemental Table {@tbl:five_pc2_table}).
More principal component word clouds can be found on our journal recommender website and within our online repository (see Software and Data Availability).

### Identifying preprints that were not linked with their corresponding publications

![
A. This violin plot shows the distribution of distances between the preprint-corresponding published version category and the preprint-randomly sampled articles category.
B. This bar chart depicts the fraction of true positives over the total number of pairs in each bin.
Each bin contains a total of 200 annotated pairs and is based on the percentiles of the preprint-published distribution.
C. This line plot shows the publication rate of preprints since bioRxiv first started.
The x-axis represents months since bioRxiv started and the y-axis represents the proportion of preprints published.
The light blue line represents the publication rate estimated by Abdill et al. [@doi:10.7554/eLife.45133].
The dark blue line represents the updated publication rate without missing links added while the dark green line is the updated publication rate with missing links added.
The horizontal lines represent the overall proportion of preprints that are published.
](https://raw.githubusercontent.com/danich1/annorxiver/f9d8861e3d60afa878c5d0088b7502adae15ebe3/figure_generation/output/figure_three_panels.svg){#fig:preprint_links_panels width="100%"}

Many journals require that authors update preprints with links to the published version of their article.
This is accomplished in two ways: _bioRxiv_ may detect the link and automatically add it or authors may notify _bioRxiv_ that their preprint was published.
Sproadically, there are cases where _bioRxiv_ may miss detecting a link or authors may forget to notiy _bioRxiv_ of their recent publcation.
These missing links can make it more difficult to identify the latest version of scientific manuscripts and estimate the fraction of articles that are eventually published [@doi:10.7554/eLife.45133].
We used distance in the document space to identify preprints without an annotated publication but contained very similar content to published articles.
We found that distances between preprints and their corresponding published versions were lower than preprints paired with a random article published in the same journal (Figure {@fig:preprint_links_panels}A).
This observation suggests that pairs with low embedding distances could be considered a true match, so we separated articles into quantiles based on the distribution of distances between true preprint-publication pairs.
We curated 50 potential preprint-publication pairs from each of four quantiles and achieved a high inter-rater reliability of 91.7% (Cohen's Kappa [@doi:10.1177/001316446002000104]) for this task.
Out of these two hundred pairs we found that approximately 98% of pairs with an embedding distance in the 0-25th and 25th-50th percentile bins were true matches (Figure {@fig:preprint_links_panels}B).
These two bins contained 1,720 preprint-article pairs, suggesting that many preprints have been published but not previously connected with their published versions.

We overlaid these new annotations onto existing annotations to reassess the overall preprint publication rate reported by Abdill et al. [@doi:10.7554/eLife.45133].
Our filtering criteria were intentionally stringent, so the increased estimate of publication rate amounts to a few percent (Figure {@fig:preprint_links_panels}C).
Many of these missed annotations were for preprints posted in the 2017-2018 interval.
Compared to preprints published in 2019 and later, the preprints posted in 2017-2018 are old enough to have a high chance of being published; however, it is interesting that the rate for older preprints was not observed to be higher.

### Factors that affect the time between preprinting and publication

![
A.This hexbin plot depicts the amount of time it takes a preprint to be published against the distances of a preprint's first version and its corresponding published version.
The x-axis represents the Euclidean distance between document representations, while the y-axis represents the number of days elapsed between a preprint posted on bioRxiv and the time a preprint is published.
The color bar on the right represents the density of each hexbin in this plot where more dense regions have a brighter color compared to their counterparts.
B.This violin plot depicts the amount of time it takes a preprint to be published against the number of versions posted for a specific preprint.
The x-axis represents the number of different versions a preprint has on bioRxiv, while the y-axis represents the number of days elapsed between a preprint posted on bioRxiv and the time a preprint is published.
Inside each violin is a boxplot for the respective preprint version distribution.
C.This bargraph depicts the amount of time it takes to get half of the total number of preprints published.
The x-axis reprints days until 50% of preprints are published and the y-axis reprints the different preprint categories.
The error bars represent 95% confidence intervals for each preprint's category half-life.
](https://raw.githubusercontent.com/danich1/annorxiver/232ab6f9f29f283070ed08b88a421b3bedaf5cbd/figure_generation/output/figure_four_panels.svg){#fig:publication_delay_panels width="100%"}

Preprints undergo multiple review checkpoints before they are published within a journal [@doi:10.1002/nop2.51].
Oftentimes these checkpoints may result in rejection or revisions requested by a reviewer [@doi:10.1002/nop2.51].
These negative outcomes result in authors may having to drastically edit their preprint, which greatly impedes a preprint reaching a published endpoint.
We sought to quantify the extent to which preprints are stalled when faced with a setback from the peer-review process.
On average preprints are delayed approximately 16 days for every distance unit change (Figure {@fig:publication_delay_panels}A).
We found that the average distance between two preprints' in the bioinformatics category was 5.068, which suggests that a single distance unit represents a fifth of a preprint's total text being changed.
Sometimes preprints have to undergo drastic revisions that result in a new version being created.
We found that on average it takes 51 days for authors to construct a new version of a preprint (Figure {@fig:publication_delay_panels}B).
Both the document distance trend and the version number trend confirm that the larger the revision the longer it takes for a preprint to be published.

Preprints in certain categories take less time to publish than others.
we sought to quantify the time each category takes to publish half their total number of preprints.
Every preprint category takes at least 124 days to publish half of their respective preprints (Figure {@fig:publication_delay_panels}C).
Categories that took the least amount of time were microbiology and zoology, while scientific communication and education took the most time (Figure {@fig:publication_delay_panels}C).
Overall, this suggests that preprints in the microbiology and zoology categories may face less peer-review setbacks compared to other categories.

### Recommending Journals Based on Preprint Representation

We sought to identify journals that might publish a preprint based on the text of a paper.
We trained two different classifiers to predict the journal endpoints for already published papers.
One classifier uses the nearest journal centroids, which attempts to capture the topic area of a journal.
The other classifier aims to be more granular and recommends journals based on close proximity of individual papers.
Both classifiers achieved a substantial increase over the random baseline; however, our predictors are not perfect (Supplemental Figure {@fig:knn_auc}).
This is expected as our dataset contains 2516 different journals where some journals publish papers that cover very specific topic while others publish papers that have a broad set of covered topics.
Our journal centroid classifier performed better than the nearest paper classifier on the held out test set (Supplemental Figure {@fig:knn_auc}).
Overall, our software provides a starting point for authors to use the text of their preprints to identify potentially suitable publication venues.

![
Here is the workflow of the journal recommender web-app.
Starting with the homescreen users can paste in a _bioRxiv_ or _medRxiv_ DOI, which sends a request to biorxiv or medrxiv (A).
Next our app preprocesses the preprint and returns a listing of the top ten most similar papers (B) and the top ten closest journals to the query (C).
Following the listing, our app manually plots the preprint query onto the Pubmed Central Landscape (D).
Lastly, users can click on a square within the landscape, which will show bin statistics as well as associated word-odd ratios (E).
](images/journal_recommender_workflow.png){#fig:journal_rec_workflow width="100%"}

We constructed an online app that provides users with journal suggestions based on their preprint content.
Users supply DOIs from _bioRxiv_ or _medRxiv_.
The application then downloads the article, converts the PDF to text, calculates a document embedding score, and returns the ten papers and journals with the most similar representations in the embedding space.
It also embeds the document into the overall PMC landscape for visualization and allows the user to examine principal components and term enrichment for each bin within the landscape (Figure {@fig:journal_rec_workflow}).
